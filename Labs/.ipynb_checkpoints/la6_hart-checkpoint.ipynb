{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Lab-6\" data-toc-modified-id=\"Lab-6-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Lab 6</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Questions\" data-toc-modified-id=\"The-Questions-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>The Questions</a></span></li><li><span><a href=\"#The-Code\" data-toc-modified-id=\"The-Code-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>The Code</a></span><ul class=\"toc-item\"><li><span><a href=\"#MNIST\" data-toc-modified-id=\"MNIST-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>MNIST</a></span><ul class=\"toc-item\"><li><span><a href=\"#kNN\" data-toc-modified-id=\"kNN-1.2.1.1\"><span class=\"toc-item-num\">1.2.1.1&nbsp;&nbsp;</span>kNN</a></span></li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-1.2.1.2\"><span class=\"toc-item-num\">1.2.1.2&nbsp;&nbsp;</span>Logistic Regression</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6\n",
    "This lab should be done with MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Write a paragraph on how k nearest neighbors could be applied to MNIST.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approaching the problem of categorizing the MNIST handwriting set using the kNN algorithm presents several interesting challenges and can seem overwhelming until they are broken apart into simple actionabe steps that form a data pipeline for transforming the data.\n",
    "\n",
    "\n",
    "When we look at the data we find that each instance is a 28x28 image. Flattening an image into a 1-dimensional array yields a 1x784 array, which means 784 features per instance. These features are represented by a matrix $X$. The class/value of each image (the digits 0-9) comprise a vector $Y$.\n",
    "\n",
    "\n",
    "In order to train the model we need to select optimal hyperparameters. In particular we need to select the optimal value k, which is the distance around each instance to search for nearest neighbors OR in other terms, the number of neighbors to consult. In addition to the number of neighbors to consult, with sci-kit learn we have the option to specify how we weight our parameters: in a nutshell we can choose to consider all weights equally or to base our weights on the distance between a oint and its neighbors. The best way to select these hyperparameters is to try a variety of hyperparameter values using cross-validation, keeping in mind that more neightbors (high k-value) means greater chances of over-fitting. In order to help prevent this we can use cross-validation in doing our hyperparameter search.\n",
    "\n",
    "\n",
    "All that is left to do once we've found optimal hyperparameters is to finish training and testing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Write a paragraph on how logistic regression could be applied to MNIST (you can use one vs all)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T22:03:14.678061Z",
     "start_time": "2019-02-19T22:03:14.666225Z"
    }
   },
   "source": [
    "In order to use Logistic Regression to classify the MNIST handwritten digit dataset we have to use a multi-class (as opposed to binary) classification model. In a nutshell, multi-class logistic regression comes down to linear algebra. Each class is represented by a hyer-plane (which is a higher-simensional analog to the line that we optimize in basic linear regression) onto which datapoints are projected. The hyperplane parameters are tuned in order to minimize the error between each hyperplane and training instances in their respective classes. Optimizing these parameters requires gradient descent (be it stochasitc, mini-batch, or batch) and the selection of an error/loss function that can be written in convex form. Our hyperparameters will be associated with this gradient descent, in particular the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Explain how you would find the best k for KNN. You can use Weka (instance-based classifiers) or KNeighborsClassifier() in sklearn (P.100). In particular, explain nested cross-validation, so that you are not testing the accuracy on the data you used to choose k. Perform the hyper parameter tuning.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested cross-validation splits the data into N-folds, then iterates in a round-robin fashion leaving one fold out at each step to be used for testing.\n",
    "\n",
    "I chose to perform several trials hyperparameter tuning on small samples of the dataset (5 trials on 5% of the data each) and found that each trial almost unanymously returned the best `k` as being equal to `4`, and they our beights should be distance-based. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. You should use regularization with logistic regression, so that means you will choose the regularization parameters based on nested cross-validation. Perform the hyper parameter tuning.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. Repeat steps 3 and 4 for multiple datasets.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the very real problems that this dataset presents is that the dataset is very large and complex, relative to say your average spreadsheet. With 42000 entries in the dataset the complexity and computation time of the problem can blow up very quickly for our purposes. In order to reduce compute time for cross-validation we can run several trials on samples of the train set, then measure the central tendency of those sampled trials and choose our hyperparameters that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T23:59:36.203542Z",
     "start_time": "2019-02-21T23:59:35.967861Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder as ohe\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T20:54:14.180643Z",
     "start_time": "2019-02-21T20:53:54.464899Z"
    }
   },
   "outputs": [],
   "source": [
    "mnist = pd.read_csv(\"data/mnist/train.csv\").values # Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T20:54:14.191721Z",
     "start_time": "2019-02-21T20:54:14.183696Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784) (42000,)\n"
     ]
    }
   ],
   "source": [
    "mnist_x, mnist_y = mnist[:, 1:], mnist[:,0] # separate target from pixels\n",
    "print(mnist_x.shape, mnist_y.shape)\n",
    "# Outputs: (42000, 784) (42000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T20:54:14.236191Z",
     "start_time": "2019-02-21T20:54:14.212946Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define an encoder to encode each class of digit as a numeric integer value\n",
    "ohencoder = ohe(sparse=False, categories='auto')\n",
    "\n",
    "# reshape training values\n",
    "mnist_y = ohencoder.fit_transform(mnist_y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T22:54:05.072282Z",
     "start_time": "2019-02-21T22:17:59.014254Z"
    }
   },
   "outputs": [],
   "source": [
    "# perform nested cross validation on multiple subsamples of the data\n",
    "# in order to reduce compute time and optimize hyperparams\n",
    "\n",
    "N_TRIALS = 5 # number of random trials\n",
    "n_samples = int(len(mnist_x)*0.05) # specify percent of dataset to sample each trial\n",
    "sample_indices = np.arange(n_samples)\n",
    "scores = [] # variable to contain scored from each trial\n",
    "\n",
    "for i in range(N_TRIALS):\n",
    "    # select a random sample from the data\n",
    "    rng = np.random.RandomState(42+i)  # reproducible results with a fixed seed\n",
    "    rng.shuffle(sample_indices)\n",
    "    x_shuff = mnist_x[sample_indices]\n",
    "    y_shuff = mnist_y[sample_indices]\n",
    "    \n",
    "    # Define the range of hyperparameters to test/optimize\n",
    "    grid_params = {\"n_neighbors\": range(3, 9),\n",
    "                   \"weights\": [\"uniform\", \"distance\"]}\n",
    "\n",
    "    # Use the kNN classifier\n",
    "    knn = KNeighborsClassifier()\n",
    "\n",
    "    # Provides train/test indices to split data in train/test sets. \n",
    "    # Split dataset into k consecutive folds\n",
    "    # Each fold is then used once as a validation.\n",
    "    # The k - 1 remaining folds form the training set.\n",
    "    cv = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    # Nested cross-validation with parameter optimization and max core utilization\n",
    "    # note that `n_jobs=-1` employs all available cores\n",
    "    grid_search = GridSearchCV(knn, param_grid=grid_params, cv=cv, n_jobs=-1)\n",
    "    grid_search.fit(X=x_shuff, y=y_shuff)\n",
    "    nested_score = cross_val_score(grid_search, X=x_shuff, y=y_shuff, cv=cv)\n",
    "    \n",
    "    # store best params and their respective scores\n",
    "    nested_scores.append([ grid_search.best_params_ , nested_score ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T22:55:28.427735Z",
     "start_time": "2019-02-21T22:55:28.420797Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([0.85119048, 0.79761905, 0.81547619, 0.85714286, 0.85119048])],\n",
       " [{'n_neighbors': 4, 'weights': 'distance'},\n",
       "  array([0.83333333, 0.81547619, 0.8452381 , 0.82738095, 0.83333333])],\n",
       " [{'n_neighbors': 3, 'weights': 'uniform'},\n",
       "  array([0.83333333, 0.82142857, 0.80952381, 0.82142857, 0.78571429])],\n",
       " [{'n_neighbors': 4, 'weights': 'distance'},\n",
       "  array([0.86309524, 0.7797619 , 0.80357143, 0.85714286, 0.81547619])],\n",
       " [{'n_neighbors': 4, 'weights': 'distance'},\n",
       "  array([0.90238095, 0.91904762, 0.88571429, 0.88333333, 0.86904762])],\n",
       " [{'n_neighbors': 4, 'weights': 'distance'},\n",
       "  array([0.87857143, 0.88333333, 0.88333333, 0.9       , 0.88571429])],\n",
       " [{'n_neighbors': 4, 'weights': 'distance'},\n",
       "  array([0.91666667, 0.86428571, 0.89285714, 0.88095238, 0.89285714])],\n",
       " [{'n_neighbors': 4, 'weights': 'distance'},\n",
       "  array([0.86190476, 0.90714286, 0.8952381 , 0.86190476, 0.88809524])],\n",
       " [{'n_neighbors': 4, 'weights': 'distance'},\n",
       "  array([0.88333333, 0.87619048, 0.92142857, 0.86904762, 0.87142857])]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test scores almost unanymously agree that we want n_neighbors=4 and weights='distance'\n",
    "nested_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T23:05:35.412168Z",
     "start_time": "2019-02-21T23:03:18.146375Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the test-set\n",
    "test = pd.read_csv(\"data/mnist/train.csv\").values\n",
    "test_x, test_y = test[:, 1:], test[:,0]\n",
    "\n",
    "# predict the values\n",
    "predictions = grid_search.predict(test_x)\n",
    "\n",
    "# encode test_y so we can compare predictions to test labels\n",
    "test_y_encoded = ohencoder.fit_transform(test_y.reshape(-1,1))\n",
    "test_y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T23:28:44.437074Z",
     "start_time": "2019-02-21T23:28:44.293148Z"
    }
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "for i, val in enumerate(predictions):\n",
    "    if False in (val == test_y_encoded[i]): incorrect+=1\n",
    "    else: correct+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T23:38:25.911648Z",
     "start_time": "2019-02-21T23:38:25.907660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42000"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check, make sure we have 42000 results\n",
    "correct+incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T23:38:35.101721Z",
     "start_time": "2019-02-21T23:38:35.093742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9079285714285714"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nearly 91% accuracy. Pretty good.\n",
    "correct/(42000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T01:00:25.674280Z",
     "start_time": "2019-02-22T01:00:25.668246Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T01:00:39.370011Z",
     "start_time": "2019-02-22T01:00:30.443607Z"
    }
   },
   "outputs": [],
   "source": [
    "mnist = pd.read_csv(\"data/mnist/train.csv\").values # Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T01:00:39.408907Z",
     "start_time": "2019-02-22T01:00:39.372005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784) (42000,)\n"
     ]
    }
   ],
   "source": [
    "mnist_x, mnist_y = mnist[:, 1:], mnist[:,0] # separate target from pixels\n",
    "print(mnist_x.shape, mnist_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T01:00:39.425328Z",
     "start_time": "2019-02-22T01:00:39.410902Z"
    }
   },
   "outputs": [],
   "source": [
    "logReg = LogisticRegression(solver='lbfgs', multi_class='multinomial', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T01:01:36.317Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "clf = GridSearchCV(logReg, param_grid, n_jobs=-1)\n",
    "\n",
    "clf = clf.fit(mnist_x, mnist_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T00:17:44.057848Z",
     "start_time": "2019-02-22T00:16:52.312098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=-1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg.fit(mnist_x, mnist_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T00:17:48.614639Z",
     "start_time": "2019-02-22T00:17:48.367301Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = logReg.predict(mnist_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T00:17:58.988930Z",
     "start_time": "2019-02-22T00:17:49.994991Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the test-set\n",
    "test = pd.read_csv(\"data/mnist/train.csv\").values\n",
    "test_x, test_y = test[:, 1:], test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T00:18:00.640443Z",
     "start_time": "2019-02-22T00:18:00.396083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9380714285714286\n"
     ]
    }
   ],
   "source": [
    "# Nearly 94% accuracy and wayyyyy lower computation time!\n",
    "score = logReg.score(test_x, test_y)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
