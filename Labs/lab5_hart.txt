I produced a synthetic linear dataset and compared gradient descent, linear regression, and a line that is the ideal central tendency.

They all has comparable R2 scores. Regression is the best, followed by GD, then the ideal line.


-- BEGIN CODE --

# Essential imports
import numpy as np
import statistics as st
import matplotlib.pyplot as plt
%matplotlib inline

# import jtplot submodule from jupyterthemes to style our plots
from jupyterthemes import jtplot
jtplot.style()

# Import learning tools
from sklearn.linear_model import  LinearRegression
from sklearn.metrics import mean_squared_error, r2_score


# probability density function of a gaussian random variable
# note that this is a normal distrubution, so (mean = median = mode)
p_g = lambda x, mean, variance : 1/np.sqrt(2*np.pi*variance)*np.exp(-((x-mean)**2)/(2*variance))


# hand code a quadratic function using the Gaussian Probability function
# so we can compare to our regression/interpolation
linear_model = lambda x : (0.5)*x + p_g(x, 10, 2)

# function that generates a set of points that are a random, constrained distance
# from our ideal quadratic curve
data_gen = lambda x : (0.5)*x + np.random.normal(0,1)

# generate our input points in the x-dimension
x = np.linspace(-3,3,100)

# generate our synthetic dataset (aka the y-values of scatterplot points)
data___ = [data_gen(i) for i in x]

# prepare synthetic dataset graph for regression
#x = np.linspace(-3,3,100)
x = x[:, np.newaxis]

y = np.asarray(data___)
y = y[:, np.newaxis]

# linear fit
model = LinearRegression()
model.fit(x, y)
y_pred = model.predict(x)

# ideal polynomial fit
y_ideal = linear_model(x)


# plot it
plt.figure(figsize=(15,10))

    # raw synthetic dataset
plt.scatter(x, y)
    # linear fit
plt.plot(x, y_pred, '.', color='purple')
    # hand-coded quadratic function
plt.plot(x, y_ideal, '--', linewidth=3, color='y')


    # hand-coded gradient desent
batch_grad_descent = lambda m, X, y, theta: (2/m) * X.T.dot(X.dot(theta) - y)

eta = 0.1
n_iterations = 1000
m = 100
theta = 1

for iteration in range(n_iterations):
    gradients = batch_grad_descent(m, x, y, theta)
    theta = theta - eta * gradients
    
plt.plot(x, theta*x, c='r', linewidth=6)



# calculate gradient descent
print("Metrics of gradient descent:")
rmse = np.sqrt(mean_squared_error(y,y_pred))
r2 = r2_score(y,theta*x)
print("  RMSE: ", rmse)
print("  R2: ", r2)
print("-------------------------------------")

# calculate regression metrics
print("Metrics of regression:")
rmse = np.sqrt(mean_squared_error(y,y_pred))
r2 = r2_score(y,y_pred)
print("  RMSE: ", rmse)
print("  R2: ", r2)
print("-------------------------------------")

# calculate ideal metrics
print("Metrics of ideal equation:")
rmse = np.sqrt(mean_squared_error(y,y_ideal))
r2 = r2_score(y,y_ideal)
print("  RMSE: ", rmse)
print("  R2: ", r2)

